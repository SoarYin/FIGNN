The code correspode to the following article in IEEE TIM. Please cite this if this work is helpful.

Y. Wang, T. Yin, S. Zhu, X. Ma, L. Liang and J. Guo, "FIGNN: Fuzzy Inference-guided Graph Neural Network for Fault Diagnosis in Industrial Processes," in IEEE Transactions on Instrumentation and Measurement, doi: 10.1109/TIM.2025.3575985.

This article is now avaliable in early access: https://doi.org/10.1109/TIM.2025.3575985

Abstract: Intelligent fault diagnosis in industrial systems is important for improving production efficiency. The complex interactions between senor units could be represented as graph and are beneficial for identifying the operational status of industrial systems. Recently, graph neural networks (GNNs) have attracted widespread attention and achieved satisfactory performance in fault diagnosis. The existing GNNs-based fault diagnosis methods with neighbor information aggregation and MLPs classifier mainly extract features strongly correlated with labels, rather than causal features, limiting the reliability and interpretability of the diagnostic results. To address these challenges, this article proposes a fuzzy inference guided graph neural network (FIGNN) for fault diagnosis. First, a fuzzy inference-guided information aggregation is skillfully designed to extract causal features by fuzzy rules, which is capable of distinguishing normal and fault characteristics. Then, the FIGNN introduces additional conclusion nodes as classifiers, achieving the explicit inference interactions between final fault and sensor signals. Finally, to enhance the reliability and flexibility of FIGNN, a network pruning-based graph structure construction approach is developed, which effectively utilizes inference relationships to adaptively construct a more accurate graph. Experimental results on the Tennessee Eastman process and the Three-Phase Flow process show the significant performance improvements of 4.18% and 0.81% and strong reliability of the proposed FIGNN in complex industrial fault diagnosis. 

Theoretical details can be viewed in the article, which is now available in an early access version(Jun 3rd 2025)